{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the final code that was used for the Kaggle Submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Spark NLP version\n",
       "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
       "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Import SPARK NLP and NLTK print version\n",
    "import sparknlp\n",
    "print(\"Spark NLP version\")\n",
    "sparknlp.version()\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from pyspark.sql.functions import col, expr, when\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import functions as sf\n",
    "\n",
    "#Import relevant packages for pipeline\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">(3487331, 13)\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load in one of the tables (add type field to split out kaggle set after)\n",
    "df1 = spark.sql(\"select 'class' as type, * from default.video_games_5\")\n",
    "df2 = spark.sql(\"select 'class' as type,* from default.books_5_small\")\n",
    "df3 = spark.sql(\"select 'class' as type,* from default.home_and_kitchen_5_small\")\n",
    "df4 = spark.sql(\"select 'kaggle' as type,*, 0 as label from default.reviews_kaggle\")\n",
    "\n",
    "#fill nulls from summary field in kaggle set (must keep all rows for submission)\n",
    "df4 = df4.na.fill('NULLS')\n",
    "\n",
    "#join base tables\n",
    "df_complete = df1.union(df2).union(df3)\n",
    "print((df_complete.count(), len(df_complete.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Before duplication removal:  3487331\n",
       "After duplication removal:  3199174\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Drop duplicates\n",
    "print(\"Before duplication removal: \", df_complete.count())\n",
    "#df_distinct = df.dropDuplicates(['reviewerID', 'asin'])\n",
    "df_distinct = df_complete.dropDuplicates(['reviewerID', 'asin'])\n",
    "print(\"After duplication removal: \", df_distinct.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">helpful review:  580477\n",
       "unhelpful review:  2618697\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_help = df_distinct.filter(col(\"label\") == 1)\n",
    "df_unhelp = df_distinct.filter(col(\"label\") == 0)\n",
    "print(\"helpful review: \", df_help.count())\n",
    "print(\"unhelpful review: \",df_unhelp.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#randomly sample unhelpful \n",
    "df_unhelp_samp = df_unhelp.sample(fraction=0.23, seed=4)\n",
    "df_full_samp = df_help.union(df_unhelp_samp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+-----+--------+-------+--------+--------------------+--------------------+-----+\n",
       " type|reviewID|overall|verified|          reviewText|             summary|label|\n",
       "+-----+--------+-------+--------+--------------------+--------------------+-----+\n",
       "class|  314824|    2.0|   false|(-) No single pla...|A competent, if c...|    1|\n",
       "class|  288817|    5.0|   false|I have owned two ...|Best Pressure Coo...|    1|\n",
       "class|  315401|    5.0|   false|We received the r...|Fun Product, read...|    1|\n",
       "class| 1935551|    5.0|    true|Great shear curta...|          Five Stars|    1|\n",
       "class|  299104|    2.0|    true|I was very disapp...| Huge Disappointment|    1|\n",
       "+-----+--------+-------+--------+--------------------+--------------------+-----+\n",
       "only showing top 5 rows\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Drop from Amazon datazset\n",
    "drop_list = ['vote', 'reviewTime', 'reviewerID', 'asin', 'reviewerName','unixReviewTime']\n",
    "df = df_full_samp.select([column for column in df_full_samp.columns if column not in drop_list])\n",
    "\n",
    "#drop from Kaggle set to match rows/columns\n",
    "drop_list = ['reviewTime', 'reviewerID', 'asin', 'reviewerName','unixReviewTime']\n",
    "df4_drop = df4.select([column for column in df4.columns if column not in drop_list])\n",
    "\n",
    "#join datasets for cleaning\n",
    "df_combined = df.union(df4_drop)\n",
    "\n",
    "#set combined as working df\n",
    "df = df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#correct datatype of verified feature\n",
    "df = df.withColumn(\"verified\", when(col(\"verified\") == \"true\",1)\n",
    "      .when(col(\"verified\") == \"false\",0)\n",
    "      .otherwise(\"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">New number of NAs/Nulls\n",
       "0\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "#from pyspark.sql.functions import concat_ws,col\n",
    "df_joined = df.withColumn('joined_column', \n",
    "                    sf.concat(sf.col('summary'),sf.lit(' '), sf.col('reviewText')))\n",
    "\n",
    "#Drop Rows with Null in Joined Column\n",
    "df_joined=df_joined.filter(sf.col('joined_column').isNotNull())        \n",
    "print(\"New number of NAs/Nulls\")\n",
    "newNA=df_joined.select('joined_column').withColumn('isNull_c',sf.col('joined_column').isNull()).where('isNull_c = True').count()\n",
    "print(newNA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">lemma_antbnc download started this may take some time.\n",
       "Approximate size to download 907.6 KB\n",
       "\r",
       "[ | ]\r",
       "[ / ]\r",
       "[OK!]\n",
       "pos_anc download started this may take some time.\n",
       "Approximate size to download 4.3 MB\n",
       "\r",
       "[ | ]\r",
       "[ / ]\r",
       "[OK!]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#Download stopwords dictionary\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "#Start SPARK NLP\n",
    "spark = sparknlp.start()\n",
    "\n",
    "#Make new Dataframe\n",
    "#df_spelling=df_remove_unwanted\n",
    "df_preprocess=df2\n",
    "\n",
    "#Create different components of the Pipeline steps-> reads in file -> tokenize -> removes stop words -> lemmatizes -> unigrams/ngrams ->outputs columns\n",
    "document = DocumentAssembler()\\\n",
    "    .setInputCol(\"joined_column\")\\\n",
    "    .setOutputCol(\"document\")\n",
    "\n",
    "tokenizer = Tokenizer() \\\n",
    "    .setInputCols([\"document\"]) \\\n",
    "    .setOutputCol(\"token\") \\\n",
    "    .setSplitChars(['-']) \\\n",
    "    .setContextChars(['(', ')', '?', '!'])\n",
    "\n",
    "remover = StopWordsCleaner()\\\n",
    "    .setInputCols([\"token\"])\\\n",
    "    .setOutputCol(\"nostopwords\")\\\n",
    "    .setStopWords(eng_stopwords)\n",
    "\n",
    "normalizer = Normalizer() \\\n",
    "     .setInputCols(['nostopwords']) \\\n",
    "     .setOutputCol('normalized') \\\n",
    "     .setLowercase(True)\n",
    "\n",
    "normalizerCaps = Normalizer() \\\n",
    "     .setInputCols(['nostopwords']) \\\n",
    "     .setOutputCol('normalized_wCaps') \\\n",
    "     .setLowercase(False)\n",
    "\n",
    "stemmer = Stemmer() \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"stem\")\n",
    "\n",
    "lemmatizer = LemmatizerModel.pretrained(name=\"lemma_antbnc\", lang=\"en\") \\\n",
    "    .setInputCols([\"normalized\"]) \\\n",
    "    .setOutputCol(\"lemmat\") \n",
    "\n",
    "ngram = NGramGenerator() \\\n",
    "            .setInputCols([\"normalized\"]) \\\n",
    "            .setOutputCol(\"ngrams\") \\\n",
    "            .setN(2) \\\n",
    "            .setEnableCumulative(True)\\\n",
    "            .setDelimiter(\"_\")\n",
    "  \n",
    "pos = PerceptronModel.pretrained('pos_anc') \\\n",
    "     .setInputCols(['document', 'normalized']) \\\n",
    "     .setOutputCol('pos')\n",
    "\n",
    "allowed_tags = ['<JJ>+<NN>', '<NN>+<NN>'] #setting ngrams to meaningful sets for the Chunker function\n",
    "\n",
    "chunker = Chunker() \\\n",
    "     .setInputCols(['document', 'pos']) \\\n",
    "     .setOutputCol('nmeaning') \\\n",
    "     .setRegexParsers(allowed_tags)\n",
    "\n",
    "finisher = Finisher() \\\n",
    "     .setInputCols(['normalized','lemmat','ngrams', 'nmeaning']) #these are the columns that will be printed to the df\n",
    "  \n",
    "pipeline = Pipeline(\n",
    "    stages = [\n",
    "        document,\n",
    "        tokenizer,\n",
    "        remover,\n",
    "        normalizer,\n",
    "        normalizerCaps,\n",
    "        stemmer,\n",
    "        lemmatizer,\n",
    "        ngram,\n",
    "        pos,\n",
    "        chunker,\n",
    "        finisher\n",
    "    ])\n",
    "\n",
    "pipelineModel = pipeline.fit(df_preprocess)\n",
    "df_preprocess = pipelineModel.transform(df_preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#NOTE YOU WILL NEED TO SELECT WHICH METHOD YOU WANT for Term Frequency Count and input that column into Line 8\n",
    "hashingTF2 = HashingTF(inputCol=\"finished_nmeaning\", outputCol=\"TermFreq_Hash_meaning\", numFeatures=2000)\n",
    "hashingTF = HashingTF(inputCol=\"finished_ngrams\", outputCol=\"TermFreq_Hash\", numFeatures=2000) #For the modelling portion \n",
    "\n",
    "#IDF inverse document frequency, is the multipilcation of TF and IDF, results in TF-IDF\n",
    "idf = IDF(inputCol=\"TermFreq_Hash\", outputCol=\"TFIDF_features\", minDocFreq=10) #SELECT WHICH COLUMN YOU WANT TO PULL FROM \n",
    "idf2 = IDF(inputCol=\"TermFreq_Hash_meaning\", outputCol=\"TFIDF_meaning_features\", minDocFreq=10) \n",
    "\n",
    "pipeline = Pipeline(stages=[hashingTF, hashingTF2, idf, idf2])\n",
    "\n",
    "tfidf_model2 = pipeline.fit(df_preprocess)\n",
    "\n",
    "tfidf_data = tfidf_model2.transform(df_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "assembler2 = VectorAssembler(\n",
    "    inputCols=[\"TFIDF_features\",\"TFIDF_meaning_features\", \"overall\", \"verified\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output2 = assembler2.transform(data_df)\n",
    "\n",
    "df_final = output2.select(\"type\",\"reviewID\",\"features\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_class = df_final.filter(col(\"type\") == \"class\")\n",
    "df_kaggle = df_final.filter(col(\"type\") == \"kaggle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#remove dummy label and type feature\n",
    "drop_list = ['label','type']\n",
    "kaggle = df_kaggle.select([column for column in df_kaggle.columns if column not in drop_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create working dataset\n",
    "dataset=df_class\n",
    "\n",
    "#train-test split\n",
    "(trainingData, testData) = dataset.randomSplit([0.7, 0.3], seed = 100)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "lrModel = lr.fit(trainingData)\n",
    "predictions = lrModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Test Area Under ROC 0.8386939958578714\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "bin_evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', bin_evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV base reg\n",
    "Kaggle_predictions_cv = lrModel.transform(kaggle)\n",
    "sub_reg_cv = Kaggle_predictions_cv.select(\"reviewID\",\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission\n",
    "display(sub_reg_cv)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "name": "1 - Team New York - Modelling Code",
  "notebookId": 2760786699351984
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
